# Human-Computer-Interaction
This project is a cost-effective, hardware-less human-system interactive system designed to help individuals with physical disabilities interact with computers independently. It leverages deep learning techniques to enable hands-free control through facial gestures and voice commands, eliminating the need for traditional input devices.

Key Features:
Facial Gesture Recognition – Tracks head and eye movements to simulate mouse operations.
Voice Command Integration – Uses speech recognition for hands-free navigation.
Adaptive Interface – Ensures usability for different users.
Cost-Effective & Scalable – Requires only a webcam and microphone.
Deep Learning-Based Approach – Ensures accuracy and reliability.
Technologies Used:
Python, OpenCV, Dlib for image processing and face tracking.
PyAutoGUI for simulating mouse actions.
SpeechRecognition for voice command processing.
NumPy & TensorFlow for computations and deep learning models.
Impact:
This system enhances accessibility by providing an alternative interaction method for individuals with motor impairments, promoting independence and digital inclusion.
